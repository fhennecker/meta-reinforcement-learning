\chapter{Issues with the reward structure}
\label{chap:reward_structure}
\begin{quotation}
\noindent ``\emph{quote}''
\begin{flushright}\textbf{author}\end{flushright}
\end{quotation}

Let us now look at why the reward of the first episode in a dual-episode
trial drops to random policy level when we know the agent is able to receive
an almost perfect reward in a single-episode trial. To understand why, we will
keep the same setting as previously \todo{ref to param table}.

\section{Training for more episodes}
There is a recurrent pattern visible in Figure~\ref{fig:20permsLR_training}.
Indeed, no matter the number of episodes in each trial, the last one will
always reach optimal or near-optimal performance while all previous episodes
will receive a reward which corresponds to following a random policy.
This is confirmed by the plots of Figure~\ref{fig:20permsLR_rewards} which
show the rewards obtained by playing several trials of 2, 5 and 10 episodes
(after having trained on trials of equal length) over all the permutations of
the training set. Only the last episode of the trial will succeed or almost
succeed.

\begin{figure}
	\centering
	\subfloat[][Training over 1 episode]{
		\includegraphics[width=0.49\linewidth]{fig/20permsLR1ep_training.pdf}}
	\subfloat[][Training over 2 episodes]{
		\includegraphics[width=0.49\linewidth]{fig/20permsLR2ep_training.pdf}}
	\\
	\subfloat[][Training over 3 episodes]{
		\includegraphics[width=0.9\linewidth]{fig/20permsLR5ep_training.pdf}}
	\caption{}
	\label{fig:20permsLR_training}
\end{figure}

\begin{figure}
	\centering
	\subfloat[][2 episodes]{
		\includegraphics[width=0.33\linewidth]{fig/20permsLR2ep_rewards.pdf}}
	\subfloat[][5 episodes]{
		\includegraphics[width=0.33\linewidth]{fig/20permsLR5ep_rewards.pdf}}
	\subfloat[][10 episodes]{
		\includegraphics[width=0.33\linewidth]{fig/20permsLR10ep_rewards.pdf}}
	\caption{}
	\label{fig:20permsLR_rewards}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{fig/gamma_impact.pdf}
	\caption{}
	\label{fig:gamma_impact}
\end{figure}

\section{Problems with a 1-per-timestep reward}
This behaviour is linked to the dynamics of the expected future reward, 
the chosen discount factor and the reward structure of the environment.
In our case, the environment gives a constant +1 reward at every timestep of
an CartPole episode. If we choose a discount factor $\gamma=0.9$, the
discounted future reward at timesteps which are more than 50 timesteps away
from the end of the episode will be almost exactly equal (see 
Figure~\ref{fig:gamma_impact}).\\

This means that if an action that defines whether the episode ends or continues
at time $t$ has to be taken before time $t-50$, there is no way to link the
end of the episode with the action that was wrongly taken. This can be turned
around to say that actions do not really matter as long as the end of the
constant reward stream is more than 50 timesteps away for $\gamma=0.9$. 

\todo{why episode longer than 200 works?}


\section{Tuning discount factor}
Show different gammas

\begin{figure}
	\centering
	\subfloat[][$\gamma=0.82$]{
		\includegraphics[width=0.49\linewidth]{fig/res_perms5ep_82.pdf}}
	\subfloat[][$\gamma=0.85$]{
		\includegraphics[width=0.49\linewidth]{fig/res_perms5ep_85.pdf}}
	\\
	\subfloat[][$\gamma=0.88$]{
		\includegraphics[width=0.49\linewidth]{fig/res_perms5ep_88.pdf}}
	\subfloat[][$\gamma=0.91$]{
		\includegraphics[width=0.49\linewidth]{fig/res_perms5ep_91.pdf}}
	\\
	\subfloat[][$\gamma=0.94$]{
		\includegraphics[width=0.49\linewidth]{fig/res_perms5ep_94.pdf}}
	\subfloat[][$\gamma=0.97$]{
		\includegraphics[width=0.49\linewidth]{fig/res_perms5ep_97.pdf}}
	\\
	\caption{}
	\label{fig:varied_gamma}
\end{figure}


